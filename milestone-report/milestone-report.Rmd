---
title: "Coursera Capstone Project Milestone Report"
subtitle: "Data Science Specialization from Johns Hopkins University"
author: "Jeffrey M. Hunter"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: cosmo
    keep_md: no
    df_print: paged
    css: css/custom.css
  pdf_document:
    toc: yes
    df_print: kable
    number_sections: false
    fig_caption: yes
    highlight: tango
    dev: pdf
  word_document:
    toc: yes
    df_print: paged
    keep_md: no
---

## Synopsis

This is the Milestone Report for week 2 of the Coursera Data Science Capstone
project.

The objective of this report is to develop an understanding of the various
statistical properties of the data set that can later be used when building the
prediction model for the final data product - the Shiny application. Using 
exploratory data analysis, this report describes the major features of the
training data and then summarizes my plans for creating the predictive model.

The model will be trained using a unified document corpus compiled from the
following three sources:

1. Blogs
1. News
1. Twitter

The provided text data are provided in four different languages. This project
will only focus on the English corpora.

## Environment Setup

Prepare the session by loading all necessary packages and clearing the global
workspace (including hidden objects).

```{r load-packages, message = FALSE, echo = TRUE}
library(knitr)
library(kableExtra)
library(stringi)
library(ggplot2)
library(gridExtra)
library(rJava)
library(dplyr)
library(tm)
library(ngram)
rm(list = ls(all.names = TRUE))
setwd("~/repos/coursera/github-assignments/coursera-data-science-capstone/milestone-report")
```

```{r setup, include = FALSE}
# set knitr options
knitr::opts_chunk$set(echo = TRUE, fig.path = 'figures/')

# free up memory and display statistics on free memory
gc()

# disable scientific notation for numbers
options(scipen = 1)

# detect OS
switch(Sys.info()[['sysname']],
    Windows = {os = "Microsoft Windows"},
    Linux = {os = "Linux"},
    Darwin = {os = "macOS"})

# knit hook to allow partial output from a code chunk
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options)) # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines) == 1) { # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```

## Load the Data

Download, unzip and load the training data.

```{r load-data, echo = TRUE}
trainURL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
trainDataFile <- "data/Coursera-SwiftKey.zip"

if (!file.exists('data')) {
    dir.create('data')
}

if (!file.exists("data/final/en_US")) {
    tempFile <- tempfile()
    download.file(trainURL, tempFile)
    unzip(tempFile, exdir = "data")
    unlink(tempFile)
}

# blogs
blogsFileName <- "data/final/en_US/en_US.blogs.txt"
con <- file(blogsFileName, open = "r")
blogs <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)

# news
newsFileName <- "data/final/en_US/en_US.news.txt"
con <- file(newsFileName, open = "r")
news <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)

# twitter
twitterFileName <- "data/final/en_US/en_US.twitter.txt"
con <- file(twitterFileName, open = "r")
twitter <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)

rm(con)
```

## Basic Data Summary

Prior to building the unified document corpus and cleaning the data, provide a
basic summary of the three text corpora including file size, number of lines,
number of characters, and number of words. Also include basic statistics on the
number of words per line (min, mean, and max).

```{r initial-data-summary-table, echo = FALSE, results = 'hold'}
# file size
fileSizeMB <- round(file.info(c(blogsFileName,
                                newsFileName,
                                twitterFileName))$size / 1024 ^ 2)

# num lines per file
numLines <- sapply(list(blogs, news, twitter), length)

# num characters per file
numChars <- sapply(list(nchar(blogs), nchar(news), nchar(twitter)), sum)

# num words per file
numWords <- sapply(list(blogs, news, twitter), stri_stats_latex)[4,]

# words per line
wpl <- lapply(list(blogs, news, twitter), function(x) stri_count_words(x))

# words per line summary
wplSummary = sapply(list(blogs, news, twitter),
             function(x) summary(stri_count_words(x))[c('Min.', 'Mean', 'Max.')])
rownames(wplSummary) = c('WPL.Min', 'WPL.Mean', 'WPL.Max')

summary <- data.frame(
    File = c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt"),
    FileSize = paste(fileSizeMB, " MB"),
    Lines = numLines,
    Characters = numChars,
    Words = numWords,
    t(rbind(round(wplSummary)))
)

kable(summary,
      row.names = FALSE,
      align = c("l", rep("r", 7)),
      caption = "Initial Data Summary") %>% kable_styling(position = "left")
```

> The source code for the above table is attached as [A.1 Basic Data Summary](#a.1-basic-data-summary) in the Appendix section.

An initial investigation of the data shows that blogs tend to have more words
per line, followed by news and then twitter which has the least words per line.
The lower number of words per line for the Twitter data is expected given that a
tweet is limited to a certain  number of characters. Even when Twitter doubled
its character count from 140 to 280 characters in 2017, research shows that only
1% of tweets hit the 280-character limit, and only 12% of tweets are longer than
140 characters. Perhaps after so many years, users were simply trained to the
140-character limit.

Another important factor identified in this initial investigation of the data
shows that the text files are fairly large. To improve processing time, only
10% of the entries for each file will be used which should allow faster 
processing time when building the model and running the Shiny application.

```{r initial-data-summary-plot, echo = FALSE, results = 'hold'}
plot1 <- qplot(wpl[[1]],
               geom = "histogram",
               main = "US Blogs",
               xlab = "Words Per Line",
               ylab = "Frequency",
               binwidth = 5)

plot2 <- qplot(wpl[[2]],
               geom = "histogram",
               main = "US News",
               xlab = "Words Per Line",
               ylab = "Frequency",
               binwidth = 5)

plot3 <- qplot(wpl[[3]],
               geom = "histogram",
               main = "US Twitter",
               xlab = "Words Per Line",
               ylab = "Frequency",
               binwidth = 1)

myPlotList = list(plot1, plot2, plot3)
do.call(grid.arrange, c(myPlotList, list(ncol = 1)))
```

> The source code for the above plot is attached as [A.2 Basic Data Histogram](#a.2-basic-data-histogram) in the Appendix section.

## Clean Sample Data

Prior to performing exploratory data analysis, the data should first be cleaned.

Pending

## Build Corpus

Pending

## Exploratory Data Analysis

Pending

## Way Forward

Pending

## Appendix

### A.1 Basic Data Summary

Basic summary of the three text corpora.

```{r initial-data-summary-table-appendix, ref.label = 'initial-data-summary-table', echo = TRUE, eval = FALSE}
```

### A.2 Basic Data Histogram

Histogram of words per line for the three text corpora.

```{r initial-data-summary-plot-appendix, ref.label = 'initial-data-summary-plot', echo = TRUE, eval = FALSE}
```



